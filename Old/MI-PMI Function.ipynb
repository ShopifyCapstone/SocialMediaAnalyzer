{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Paste Other Functions - b/c import wasn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_DfColumn(df,columnIndicator):\n",
    "    import pandas\n",
    "    tempSeries = df[columnIndicator]\n",
    "    strcat = tempSeries.str.cat(sep=', ')\n",
    "    return strcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_KeyPhrases(textInput, k = 15, version = 'Summary'):\n",
    "    #version = Summary or version = PMI\n",
    "    import nltk\n",
    "    nltk.download('punkt',quiet='true')\n",
    "    nltk.download(\"averaged_perceptron_tagger\",quiet='true')\n",
    "    nltk.download(\"wordnet\",quiet='true')\n",
    "    nltk.download(\"brown\",quiet='true')\n",
    "    from nltk.data import find\n",
    "    from nltk.tag import PerceptronTagger\n",
    "   \n",
    "\n",
    "    #setting up tagger\n",
    "    #(from http://stackoverflow.com/a/35964709)\n",
    "    PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "    AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n",
    "    tagger = PerceptronTagger(load=False)\n",
    "    tagger.load(AP_MODEL_LOC)\n",
    "    pos_tag = tagger.tag\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    \n",
    "    # This grammar is described in the paper by S. N. Kim,\n",
    "    # T. Baldwin, and M.-Y. Kan.\n",
    "    # Evaluating n-gram based evaluation metrics for automatic\n",
    "    # keyphrase extraction.\n",
    "    # Technical report, University of Melbourne, Melbourne 2010.\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "\n",
    "    def acceptable_word(word):\n",
    "        \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "        accepted = bool(2 < len(word) and word.lower() not in stopwords)\n",
    "        return accepted        \n",
    "\n",
    "    def normalise(word):\n",
    "        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "        word = word.lower()\n",
    "        word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        return word\n",
    "\n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            #can modify normalise to w.lower() if dont want to normalize word\n",
    "            term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "            yield term\n",
    "        \n",
    "    def get_nounPhrases(textInput, minWordLength = 2):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "        grammar = r\"\"\"\n",
    "\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "                  \"\"\"\n",
    "\n",
    "        chunker = nltk.RegexpParser(grammar)\n",
    "    \n",
    "        toks = nltk.word_tokenize(textInput)\n",
    "        #print(toks)\n",
    "        pos_tag = tagger.tag\n",
    "        postoks = pos_tag(toks)\n",
    "\n",
    "        tree = chunker.parse(postoks)\n",
    "        terms = get_terms(tree)\n",
    "   \n",
    "        nounPhraseList = []\n",
    "        for tid,term in enumerate(terms):\n",
    "            templist = []\n",
    "            for wid, word in enumerate(term):\n",
    "                #print(\"TID: \",tid,\" WID: \",(wid+1), word)\n",
    "                templist.append(word)\n",
    "        \n",
    "            s = \" \"\n",
    "            nounPhraseList.append(s.join(templist))\n",
    "\n",
    "        nounPhraseList = [word for word in nounPhraseList if len(word.split())>=minWordLength]\n",
    "        return nounPhraseList\n",
    "    \n",
    "    import pandas\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for nounPhrase in  get_nounPhrases(textInput):\n",
    "        #print(nounPhrase)\n",
    "        counter.update([nounPhrase])\n",
    "    if version.lower() == 'summary':       \n",
    "        topkNPdf =pandas.DataFrame([[key,value] for key,value in counter.items()],columns=['Term','Frequency'])\n",
    "        #topkNPdf = topkNPdf.reset_index(drop=True)\n",
    "\n",
    "        #if less than max (15), use correct number of key phrases\n",
    "        if topkNPdf.shape[0]<k:\n",
    "            print(\"\\n \\nTop\" ,topkNPdf.shape[0], \"key phrases (minimum phrase length = 2 ): \\n\")\n",
    "        else:\n",
    "            print(\"\\n \\nTop\" ,k, \"key phrases (minimum phrase length = 2): \\n\") \n",
    "\n",
    "\n",
    "        topkNPdf= topkNPdf.sort_values('Frequency', axis=0, ascending=False).head(k)\n",
    "        topkNPdf = topkNPdf.reset_index(drop=True)\n",
    "        return topkNPdf\n",
    "    \n",
    "    elif version.lower() == 'pmi':\n",
    "        return counter.most_common(k);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanData(df):\n",
    "    import re\n",
    "    URL_REGEX_1 = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    SHOPIFY_REGEX = r\"[S|s]hopify\"\n",
    "    HIRING_REGEX = r\"[H|h]iring\"\n",
    "    POPULARCOMMENT_REGEX = r\"\\**Most Popular Comments\\**\"\n",
    "    HISTORY_REGEX = r\"[R|r]ecent [S|s]ubmission [H|h]istory \"\n",
    "    \n",
    "    dfCopy = df.copy()\n",
    "    dfCopy['bodyCopy'] =dfCopy['body']\n",
    "    \n",
    "    #list to store ids of comments to keep \n",
    "    commentKeep =[]\n",
    "\n",
    "    for cid,comment in enumerate(dfCopy['bodyCopy']):\n",
    "        #print(cid)\n",
    "        #print(dfCopy.loc[dfCopy['body']==comment].index.values[0])\n",
    "        if (len(re.findall(URL_REGEX_1,comment))>0):\n",
    "            dfCopy['bodyCopy'][cid] = re.sub(URL_REGEX_1,'WEBSITE_FILLER',comment)\n",
    "            #print(dfCopy['bodyCopy'][cid])\n",
    "\n",
    "    # 2nd loop to check if post meets requirements after subbing in URL FILLER    \n",
    "    for cid,comment in enumerate(dfCopy['bodyCopy']):    \n",
    "        #only keep comments with links if they mention shopify w/o it occuring in link    \n",
    "        if (len(re.findall(SHOPIFY_REGEX,comment))>0):\n",
    "                #remove any post with 'Hiring' or 'Recent Submission History'\n",
    "                if(len(re.findall(HIRING_REGEX,comment))==0 and  len(re.findall(HISTORY_REGEX,comment))==0 and len(re.findall(POPULARCOMMENT_REGEX,comment))==0):\n",
    "                    commentKeep.append(cid)\n",
    "\n",
    "\n",
    "    print(\"****Done****\")\n",
    "    dfClean=df.loc[commentKeep]\n",
    "    \n",
    "    \n",
    "    return dfClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calcualte MI scores\n",
    "def miCalc(df, topK, scoreMetric = 'vader'):\n",
    "    import sklearn\n",
    "    import sklearn.metrics as metrics\n",
    "   \n",
    "    gtScore = []\n",
    "    threshold = 0\n",
    "    \n",
    "    # Convet score metric to binary values\n",
    "    for i in range(len(df)):\n",
    "        if df[scoreMetric].as_matrix()[i]>threshold:\n",
    "            gtScore.append(1)\n",
    "        else:\n",
    "            gtScore.append(0)            \n",
    "\n",
    "    # Perform MI Analysis for each word\n",
    "    miScore = []\n",
    "    for word, wordCount in topK:\n",
    "        miScore.append([word]+[metrics.mutual_info_score(gtScore, df[word].as_matrix())])\n",
    "       \n",
    "    \n",
    "    miScoredf = pandas.DataFrame(miScore).sort_values(1,ascending=0)\n",
    "    miScoredf = miScoredf.reset_index(drop = True)\n",
    "    miScoredf.columns = ['Word','MI Score']\n",
    "    return miScoredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcPMI(df, MItype = 'mi'):\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import math\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentAnalyzer\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    #from nltk.sentiment.util import *\n",
    "    from nltk import tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "   \n",
    "    df =cleanData(df)\n",
    "    \n",
    "    def commentPolarity(df):\n",
    "        commentContent = df['body'].as_matrix()\n",
    "        #Instantiation\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        pdlist = []\n",
    "        ##Assign Vader score to individual review using Vader compound score\n",
    "        ##creating a list of the reviews along with their polarity score as assigned by Vader\n",
    "        ##list containing a list where each element is [review,polarity]\n",
    "        for rownum, comment in enumerate(commentContent):\n",
    "            ss = sid.polarity_scores(comment)\n",
    "            pdlist.append([comment,ss['compound']])\n",
    "            if (rownum % 100 == 1):\n",
    "                    print(\"processed %d reviews\" % (rownum+1))\n",
    "        print(\"****Done****\")\n",
    "        commentDf = pandas.DataFrame(pdlist)\n",
    "        commentDf.columns = ['commentCol','vader']\n",
    "#         commentDf.head()\n",
    "        return commentDf\n",
    "        \n",
    "    def commentTermDF(df):\n",
    "        \n",
    "        top_k =  get_KeyPhrases(concat_DfColumn(df,'commentCol'), k = 100, version = 'pmi')\n",
    "\n",
    "        freqReview = []\n",
    "        #create a list, one entry for each review; the entry is binary list indicating whether this review as the term i or not in the top k\n",
    "        for i in range(len(df)):\n",
    "            commentKP = get_KeyPhrases(df['commentCol'][i], k = 100, version = 'pmi')\n",
    "        #     print('commentKP: ' , commentKP)\n",
    "            keyPhraseDict = {commentKP[i][0]: commentKP[i][1] for i in range(0, len(commentKP))}\n",
    "        #     print('\\n keyPhraseDict',keyPhraseDict)\n",
    "            topkinComment = [1 if  keyPhraseDict.get(word, 0) > 0 else 0 for (word,wordCount) in top_k]\n",
    "        #     print('\\n topkinComment',topkinComment)\n",
    "            freqReview.append(topkinComment)\n",
    "\n",
    "        freqReviewDf = pandas.DataFrame(freqReview)\n",
    "        dfName = []\n",
    "        for c in top_k:\n",
    "            dfName.append(c[0])\n",
    "            #print(c)\n",
    "        freqReviewDf.columns = dfName\n",
    "#         freqReviewDf.head()\n",
    "        return top_k,freqReviewDf\n",
    "    \n",
    "    \n",
    "    df1 = commentPolarity(df)\n",
    "    top_k, df2 = commentTermDF(df1)\n",
    "    finalcommentDf = df1.join(df2)\n",
    "    df_reindexed = df.reset_index(drop=True)\n",
    "    finaldf = df_reindexed.join(finalcommentDf)\n",
    "    if MItype.lower() == 'mi':\n",
    "        miDF = miCalc(finaldf, top_k, scoreMetric = 'vader')\n",
    "        return miDF\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#!pip --quiet install nltk\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#for candidate key phrase code\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "import pandas\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#location of data\n",
    "DATA_DIR = r'C:\\Users\\Jason\\MIE490 - Capstone - Shopify\\Data\\Comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we build a list of all the full paths of the files in DOCUMENTS_DIR\n",
    "import os\n",
    "file_in = []\n",
    "# os.walk - \n",
    "#Generate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory\n",
    "#in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    #print(root  + '\\n')\n",
    "    #print(files)\n",
    "    #print(files +\"\\n\")\n",
    "    filePaths = [os.path.join(root, fileName) for fileName in files if not fileName.startswith('.')]\n",
    "    #print(filePaths)\n",
    "    file_in.extend(filePaths)\n",
    "    #print(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDf = pandas.DataFrame()\n",
    "#following code courtesy of: https://www.reddit.com/r/MachineLearning/comments/33eglq/python_help_jsoncsv_pandas/\n",
    "for file in file_in:\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # remove the trailing \"\\n\" from each line\n",
    "    data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "    # each element of 'data' is an individual JSON object.\n",
    "    # i want to convert it into an *array* of JSON objects\n",
    "    # which, in and of itself, is one large JSON object\n",
    "    # basically... add square brackets to the beginning\n",
    "    # and end, and have all the individual business JSON objects\n",
    "    # separated by a comma\n",
    "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "    # now, load it into pandas\n",
    "    dataDf = dataDf.append(pandas.read_json(data_json_str),ignore_index=True)\n",
    "    #print(dataDf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>battery_bot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n&amp;amp;nbsp;  \\n\\n ^Brand | ^Model | ^(Rated C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1427847662</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_30zopt</td>\n",
       "      <td>t1_cpxa6po</td>\n",
       "      <td>t1_cpxa6mw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1432126807</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>electronic_cigarette</td>\n",
       "      <td>t5_2qmlu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>uncle_dubya</td>\n",
       "      <td></td>\n",
       "      <td>the funky uncle.</td>\n",
       "      <td>yeah, that's where i saw them. i thought about...</td>\n",
       "      <td>0</td>\n",
       "      <td>1427849075</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_30x0l7</td>\n",
       "      <td>t1_cpxb06p</td>\n",
       "      <td>t1_cpx0fy0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1432127192</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Wet_Shavers</td>\n",
       "      <td>t5_32kax</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  archived       author author_flair_css_class author_flair_text  \\\n",
       "0    False  battery_bot                   None              None   \n",
       "1    False  uncle_dubya                         the funky uncle.   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  \\n&amp;nbsp;  \\n\\n ^Brand | ^Model | ^(Rated C...                 0   \n",
       "1  yeah, that's where i saw them. i thought about...                 0   \n",
       "\n",
       "   created_utc distinguished  downs  edited ...    link_id        name  \\\n",
       "0   1427847662          None      0       0 ...  t3_30zopt  t1_cpxa6po   \n",
       "1   1427849075          None      0       0 ...  t3_30x0l7  t1_cpxb06p   \n",
       "\n",
       "    parent_id removal_reason retrieved_on  score  score_hidden  \\\n",
       "0  t1_cpxa6mw            NaN   1432126807      1         False   \n",
       "1  t1_cpx0fy0            NaN   1432127192      1         False   \n",
       "\n",
       "              subreddit subreddit_id ups  \n",
       "0  electronic_cigarette     t5_2qmlu   1  \n",
       "1           Wet_Shavers     t5_32kax   1  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3870, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Done****\n"
     ]
    }
   ],
   "source": [
    "cleanDF = cleanData(dataDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 22)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
       "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
       "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
       "       'removal_reason', 'retrieved_on', 'score', 'score_hidden', 'subreddit',\n",
       "       'subreddit_id', 'ups'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Done****\n",
      "processed 2 reviews\n",
      "processed 102 reviews\n",
      "processed 202 reviews\n",
      "processed 302 reviews\n",
      "processed 402 reviews\n",
      "processed 502 reviews\n",
      "processed 602 reviews\n",
      "processed 702 reviews\n",
      "processed 802 reviews\n",
      "processed 902 reviews\n",
      "processed 1002 reviews\n",
      "processed 1102 reviews\n",
      "processed 1202 reviews\n",
      "processed 1302 reviews\n",
      "processed 1402 reviews\n",
      "processed 1502 reviews\n",
      "processed 1602 reviews\n",
      "processed 1702 reviews\n",
      "****Done****\n"
     ]
    }
   ],
   "source": [
    "temp = calcPMI(dataDf, MItype = 'mi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>MI Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good luck</td>\n",
       "      <td>5.602753e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free ship</td>\n",
       "      <td>1.607251e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>littl bit</td>\n",
       "      <td>1.460651e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole thing</td>\n",
       "      <td>1.460651e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>front page</td>\n",
       "      <td>1.167746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bank account</td>\n",
       "      <td>1.167746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>much time</td>\n",
       "      <td>1.167746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host solut</td>\n",
       "      <td>1.167746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>free theme</td>\n",
       "      <td>1.167746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ecommerc platform</td>\n",
       "      <td>1.161278e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pci complianc</td>\n",
       "      <td>1.021439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>competitor price</td>\n",
       "      <td>1.021439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>import thing</td>\n",
       "      <td>1.021439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ecommerc websit</td>\n",
       "      <td>1.021439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good thing</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good idea</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>free trial</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>profit margin</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>last year</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>good point</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>e-commerc solut</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shopifi app store</td>\n",
       "      <td>8.752285e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>shopifi page</td>\n",
       "      <td>8.685852e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>social medium</td>\n",
       "      <td>7.837836e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>paypal button</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>wordpress theme</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>target audienc</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>first time</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>e-commerc platform</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nike etc</td>\n",
       "      <td>7.291155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>domain name</td>\n",
       "      <td>8.103851e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>potenti custom</td>\n",
       "      <td>8.103851e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>woo commerc</td>\n",
       "      <td>8.103851e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>wordpress site</td>\n",
       "      <td>8.103851e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>monthli cost</td>\n",
       "      <td>8.103851e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>shopifi store</td>\n",
       "      <td>7.157091e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>onlin store</td>\n",
       "      <td>6.360615e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>shopifi platform</td>\n",
       "      <td>4.657573e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>app store</td>\n",
       "      <td>4.594265e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>email address</td>\n",
       "      <td>4.594265e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>web develop</td>\n",
       "      <td>3.274371e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>front end</td>\n",
       "      <td>3.261756e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>first thing</td>\n",
       "      <td>3.261756e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>shopifi plugin</td>\n",
       "      <td>3.261756e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>land page</td>\n",
       "      <td>3.079205e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ecommerc site</td>\n",
       "      <td>2.146906e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>shopifi templat</td>\n",
       "      <td>1.068288e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>payment gateway</td>\n",
       "      <td>1.068288e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>long time</td>\n",
       "      <td>8.037442e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>facebook ad</td>\n",
       "      <td>8.037442e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>payment method</td>\n",
       "      <td>5.328623e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>side project</td>\n",
       "      <td>5.328623e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>home page</td>\n",
       "      <td>4.007285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>custom code</td>\n",
       "      <td>4.007285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>shopifi websit</td>\n",
       "      <td>4.007285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>great deal</td>\n",
       "      <td>4.007285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>shopifi blog</td>\n",
       "      <td>4.007285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>payment processor</td>\n",
       "      <td>1.252148e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>long run</td>\n",
       "      <td>7.695331e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blog post</td>\n",
       "      <td>7.695331e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Word      MI Score\n",
       "0            good luck  5.602753e-03\n",
       "1            free ship  1.607251e-03\n",
       "2            littl bit  1.460651e-03\n",
       "3          whole thing  1.460651e-03\n",
       "4           front page  1.167746e-03\n",
       "5         bank account  1.167746e-03\n",
       "6            much time  1.167746e-03\n",
       "7           host solut  1.167746e-03\n",
       "8           free theme  1.167746e-03\n",
       "9    ecommerc platform  1.161278e-03\n",
       "10       pci complianc  1.021439e-03\n",
       "11    competitor price  1.021439e-03\n",
       "12        import thing  1.021439e-03\n",
       "13     ecommerc websit  1.021439e-03\n",
       "14          good thing  8.752285e-04\n",
       "15           good idea  8.752285e-04\n",
       "16          free trial  8.752285e-04\n",
       "17       profit margin  8.752285e-04\n",
       "18           last year  8.752285e-04\n",
       "19          good point  8.752285e-04\n",
       "20     e-commerc solut  8.752285e-04\n",
       "21   shopifi app store  8.752285e-04\n",
       "22        shopifi page  8.685852e-04\n",
       "23       social medium  7.837836e-04\n",
       "24       paypal button  7.291155e-04\n",
       "25     wordpress theme  7.291155e-04\n",
       "26      target audienc  7.291155e-04\n",
       "27          first time  7.291155e-04\n",
       "28  e-commerc platform  7.291155e-04\n",
       "29            nike etc  7.291155e-04\n",
       "..                 ...           ...\n",
       "70         domain name  8.103851e-05\n",
       "71      potenti custom  8.103851e-05\n",
       "72         woo commerc  8.103851e-05\n",
       "73      wordpress site  8.103851e-05\n",
       "74        monthli cost  8.103851e-05\n",
       "75       shopifi store  7.157091e-05\n",
       "76         onlin store  6.360615e-05\n",
       "77    shopifi platform  4.657573e-05\n",
       "78           app store  4.594265e-05\n",
       "79       email address  4.594265e-05\n",
       "80         web develop  3.274371e-05\n",
       "81           front end  3.261756e-05\n",
       "82         first thing  3.261756e-05\n",
       "83      shopifi plugin  3.261756e-05\n",
       "84           land page  3.079205e-05\n",
       "85       ecommerc site  2.146906e-05\n",
       "86     shopifi templat  1.068288e-05\n",
       "87     payment gateway  1.068288e-05\n",
       "88           long time  8.037442e-06\n",
       "89         facebook ad  8.037442e-06\n",
       "90      payment method  5.328623e-06\n",
       "91        side project  5.328623e-06\n",
       "92           home page  4.007285e-06\n",
       "93         custom code  4.007285e-06\n",
       "94      shopifi websit  4.007285e-06\n",
       "95          great deal  4.007285e-06\n",
       "96        shopifi blog  4.007285e-06\n",
       "97   payment processor  1.252148e-06\n",
       "98            long run  7.695331e-09\n",
       "99           blog post  7.695331e-09\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
