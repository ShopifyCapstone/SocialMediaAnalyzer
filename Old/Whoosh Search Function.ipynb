{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whoosh Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function wont work if import statement is inside, must be outside\n",
    "from whoosh.analysis import Filter\n",
    "class CustomFilter(Filter):\n",
    "    # This filter will run for both the index and the query\n",
    "    is_morph = True\n",
    "    def __init__(self, filterFunc, *args, **kwargs):\n",
    "        self.customFilter = filterFunc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def __eq__(self):\n",
    "        return (other\n",
    "                and self.__class__ is other.__class__)\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if t.mode == 'query': # if called by query parser\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t\n",
    "            else: # == 'index' if called by indexer\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ShopifySearchEngine(df, userQuery):\n",
    "    #!pip --quiet install whoosh\n",
    "    from whoosh import index, writing\n",
    "    from whoosh.fields import Schema, TEXT, ID, STORED\n",
    "    from whoosh.analysis import RegexTokenizer, LowercaseFilter, StopFilter, StemFilter\n",
    "    from whoosh import qparser\n",
    "    from whoosh.qparser import QueryParser, GtLtPlugin, PhrasePlugin, SequencePlugin\n",
    "    from whoosh import scoring\n",
    "    import os, os.path # os - portable way of using operating system dependent functionality\n",
    "    import shutil #High-level file operations\n",
    "    import pandas\n",
    "    import nltk\n",
    "    \n",
    "    \n",
    "    #Defining constants for the data paths ***** MODIFY ACCORDINGLY *****\n",
    "    INDEX_DIR = r\"C:\\Users\\Jason\\MIE490 - Capstone - Shopify\\Data\\Index2\"\n",
    "    \n",
    "    \n",
    "    #BUILD SCHEMA ****\n",
    "    #schema has fields - piece of info for each doc in the index\n",
    "    customWordFilter = RegexTokenizer()|LowercaseFilter()|CustomFilter(nltk.stem.porter.PorterStemmer().stem)|CustomFilter(nltk.WordNetLemmatizer().lemmatize)\n",
    "\n",
    "    ixSchema = Schema(comment_ID = ID(stored=True),\n",
    "                         comment_Subreddit = ID(stored=True),\n",
    "                         #note analyzer is a wrapper for a tokenizer and zero or more filters -- i.e. allows you to combine them\n",
    "                         comment_Content = TEXT(analyzer = customWordFilter))\n",
    "  \n",
    "\n",
    "    #BUILD INDEX ****\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(INDEX_DIR):\n",
    "        os.mkdir(INDEX_DIR)\n",
    "\n",
    "\n",
    "#     if index exists - remove it\n",
    "#     #Return True if path is an existing directory.\n",
    "#     if os.path.isdir(INDEX_DIR):\n",
    "#         #Delete an entire directory tree; path must point to a directory\n",
    "#         shutil.rmtree(INDEX_DIR)\n",
    "#     #create the directory for the index\n",
    "#     os.makedirs(INDEX_DIR)\n",
    "\n",
    "    #initiate index - takes two inputs, the index directory and the schema for the index\n",
    "    ix = index.create_in(INDEX_DIR,ixSchema)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #INDEX COMMENTS ****\n",
    "    #creating a utility writer \n",
    "    #params: index – the whoosh.index.Index to write to.\n",
    "    #period – the maximum amount of time (in seconds) between commits.\n",
    "    #limit – the maximum number of documents to buffer before committing/between commits.\n",
    "    writer = writing.BufferedWriter(ix, period=20, limit=1000)\n",
    "    try:\n",
    "        # write each file to index\n",
    "        # enumerate returns index,value index points too --> index,a[index]\n",
    "        \n",
    "        counter1 = 0\n",
    "        for row in df.iterrows():\n",
    "            index,data = row\n",
    "            writer.add_document(comment_ID = data['name'],\n",
    "                                comment_Subreddit = data['subreddit'],\n",
    "                                comment_Content = data['body'])\n",
    "            counter1 = counter1 + 1\n",
    "            \n",
    "#             if (counter1 % 100 == 0):\n",
    "#                 print(\"already indexed:\", counter1+1)\n",
    "\n",
    "    finally:\n",
    "        # save the index\n",
    "        #print(\"done indexing\")\n",
    "        # *** Note *** -> Must explictly call close() on the writer object to release the write lock and makesure uncommited changes are saved \n",
    "        writer.close()   \n",
    "      \n",
    "    \n",
    "    #PARSE USER QUERY ****\n",
    "    \n",
    "    #in the query parser --> we pass the DEFAULT field to search and the schema of the index we are searching\n",
    "    #NOTE: Users can still specify a search on a different field in the schema via --> <fieldname>: <query>\n",
    "    qp = QueryParser(\"comment_Content\", schema=ix.schema)\n",
    "\n",
    "     #Once you have a QueryParser object, you can call parse() on it to parse a query string into a query object:\n",
    "        #default query lang: \n",
    "        #If the user doesn’t explicitly specify AND or OR clauses: \n",
    "        #by default, the parser treats the words as if they were connected by AND,\n",
    "        #meaning all the terms must be present for a document to match\n",
    "        #we will change this \n",
    "        #to phrase search \"<query>\" - use quotes\n",
    "        \n",
    "    qp.add_plugin(qparser.GtLtPlugin)   \n",
    "    #qp.remove_plugin_class(qparser.PhrasePlugin)\n",
    "    qp.add_plugin(qparser.PhrasePlugin)  \n",
    "    query = qp.parse(userQuery)\n",
    "    print(\"\\n\\n Query: \")\n",
    "    print(query)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ##IMPLEMENT SEARCHER ****\n",
    "    resultsDF = pandas.DataFrame() #creates a new dataframe that's empty to store the results comment content\n",
    "    with ix.searcher(weighting = scoring.BM25F()) as searcher:\n",
    "        queryResults = searcher.search(query, limit = None)\n",
    "        print(\"Total Number of Results:\",len(queryResults))\n",
    "        print(\"Number of scored and sorted docs in this Results object:\",queryResults.scored_length())\n",
    "        for result in queryResults:\n",
    "#             print(result)\n",
    "#             print(\"\\n\",result['comment_ID'])\n",
    "            resultsDF = resultsDF.append(df.loc[df['name']== result['comment_ID']][['name','subreddit','body']])\n",
    "            \n",
    "\n",
    "    #print(dataDf.loc[dataDf['body']==comment].index.values[0])\n",
    "        \n",
    "    return resultsDF     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "d = {'name': 't12345', 'subreddit': 'shopify','body': 'this is a test comment for shopify'}\n",
    "df = pandas.DataFrame(data=d,index = range(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a test comment for shopify</td>\n",
       "      <td>t12345</td>\n",
       "      <td>shopify</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 body    name subreddit\n",
       "0  this is a test comment for shopify  t12345   shopify"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Query: \n",
      "comment_Content:\"thi is a test\"\n",
      "\n",
      "\n",
      "\n",
      "Total Number of Results: 1\n",
      "Number of scored and sorted docs in this Results object: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t12345</td>\n",
       "      <td>shopify</td>\n",
       "      <td>this is a test comment for shopify</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name subreddit                                body\n",
       "0  t12345   shopify  this is a test comment for shopify"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShopifySearchEngine(df,\"\\\"this is a test\\\"\")\n",
    "## Right now, it supports basic boolean search using AND and/or OR using \"Jakob AND and AND Jack OR or Frank\" ,\n",
    "## only capitlized ones get treated as OR and AD\n",
    "# default \"Jack Jones\" --> treated as Jack AND Jones\n",
    "## Can specify phrase as above using the \\\" \\\"\n",
    "## I think the more advanced way is to build the user query from the bottom up but there is no good examples online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Query: \n",
      "((comment_Content:jack AND comment_Content:and AND comment_Content:jone) OR comment_Content:frank OR comment_Content:shopifi)\n",
      "\n",
      "\n",
      "\n",
      "Total Number of Results: 1\n",
      "Number of scored and sorted docs in this Results object: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t12345</td>\n",
       "      <td>shopify</td>\n",
       "      <td>this is a test comment for shopify</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name subreddit                                body\n",
       "0  t12345   shopify  this is a test comment for shopify"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShopifySearchEngine(df,\"Jack AND and AND Jones OR Frank OR Shopify\")\n",
    "## Right now, it supports basic boolean search using AND and/or OR using \"Jakob AND and AND Jack OR or Frank\" ,\n",
    "## only capitlized ones get treated as OR and AD\n",
    "# default \"Jack Jones\" --> treated as Jack AND Jones\n",
    "## Can specify phrase as above using the \\\" \\\"\n",
    "## I think the more advanced way is to build the user query from the bottom up but there is no good examples online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
