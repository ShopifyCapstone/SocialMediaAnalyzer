{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.analysis import Filter\n",
    "from whoosh import index as Index\n",
    "from whoosh import writing\n",
    "from whoosh.fields import Schema, TEXT, ID, STORED\n",
    "from whoosh.analysis import RegexTokenizer, LowercaseFilter, StopFilter, StemFilter\n",
    "from whoosh import qparser\n",
    "from whoosh.qparser import QueryParser, GtLtPlugin, PhrasePlugin, SequencePlugin\n",
    "from whoosh import scoring\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os, os.path # os - portable way of using operating system dependent functionality\n",
    "import shutil #High-level file operations\n",
    "\n",
    "import tkinter\n",
    "from tkinter import *\n",
    "import tkinter.font as tkFont\n",
    "import tkinter.ttk as ttk\n",
    "from tkinter.ttk import Combobox,Treeview,Scrollbar\n",
    "\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "import pandas\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt',quiet='true')\n",
    "nltk.download(\"averaged_perceptron_tagger\",quiet='true')\n",
    "nltk.download(\"wordnet\",quiet='true')\n",
    "nltk.download(\"brown\",quiet='true')\n",
    "\n",
    "def get_KeyPhrases(textInput, k = 15, version = 'Summary'):\n",
    "    #version = Summary or version = PMI\n",
    "    \n",
    "    #setting up tagger\n",
    "    #(from http://stackoverflow.com/a/35964709)\n",
    "    PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "    AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n",
    "    tagger = PerceptronTagger(load=False)\n",
    "    tagger.load(AP_MODEL_LOC)\n",
    "    pos_tag = tagger.tag\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    # This grammar is described in the paper by S. N. Kim,\n",
    "    # T. Baldwin, and M.-Y. Kan.\n",
    "    # Evaluating n-gram based evaluation metrics for automatic\n",
    "    # keyphrase extraction.\n",
    "    # Technical report, University of Melbourne, Melbourne 2010.\n",
    "    \n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "\n",
    "    def acceptable_word(word):\n",
    "        \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "        accepted = bool(2 < len(word) and word.lower() not in stopwords)\n",
    "        return accepted        \n",
    "\n",
    "    def normalise(word):\n",
    "        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "        word = word.lower()\n",
    "        word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        return word\n",
    "\n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            #can modify normalise to w.lower() if dont want to normalize word\n",
    "            term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "            yield term\n",
    "        \n",
    "    def get_nounPhrases(textInput, minWordLength = 2):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "        grammar = r\"\"\"\n",
    "\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "                  \"\"\"\n",
    "\n",
    "        chunker = nltk.RegexpParser(grammar)\n",
    "    \n",
    "        toks = nltk.word_tokenize(textInput)\n",
    "        #print(toks)\n",
    "        pos_tag = tagger.tag\n",
    "        postoks = pos_tag(toks)\n",
    "\n",
    "        tree = chunker.parse(postoks)\n",
    "        terms = get_terms(tree)\n",
    "   \n",
    "        nounPhraseList = []\n",
    "        for tid,term in enumerate(terms):\n",
    "            templist = []\n",
    "            for wid, word in enumerate(term):\n",
    "                #print(\"TID: \",tid,\" WID: \",(wid+1), word)\n",
    "                templist.append(word)\n",
    "        \n",
    "            s = \" \"\n",
    "            nounPhraseList.append(s.join(templist))\n",
    "\n",
    "        nounPhraseList = [word for word in nounPhraseList if len(word.split())>=minWordLength]\n",
    "        return nounPhraseList\n",
    "    \n",
    "    counter = Counter()\n",
    "    for nounPhrase in  get_nounPhrases(textInput):\n",
    "        #print(nounPhrase)\n",
    "        counter.update([nounPhrase])\n",
    "    if version.lower() == 'summary':       \n",
    "        topkNPdf =pandas.DataFrame([[key,value] for key,value in counter.items()],columns=['Term','Frequency'])\n",
    "        #topkNPdf = topkNPdf.reset_index(drop=True)\n",
    "\n",
    "        #if less than max (15), use correct number of key phrases\n",
    "        if topkNPdf.shape[0]<k:\n",
    "            print(\"\\n \\nTop\" ,topkNPdf.shape[0], \"key phrases (minimum phrase length = 2 ): \\n\")\n",
    "        else:\n",
    "            print(\"\\n \\nTop\" ,k, \"key phrases (minimum phrase length = 2): \\n\") \n",
    "\n",
    "\n",
    "        topkNPdf= topkNPdf.sort_values('Frequency', axis=0, ascending=False).head(k)\n",
    "        topkNPdf = topkNPdf.reset_index(drop=True)\n",
    "        return topkNPdf\n",
    "    \n",
    "    elif version.lower() == 'pmi':\n",
    "        return counter.most_common(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_KeyPhrases('. '.join(df2['body'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomFilter(Filter):\n",
    "    # This filter will run for both the index and the query\n",
    "    is_morph = True\n",
    "    def __init__(self, filterFunc, *args, **kwargs):\n",
    "        self.customFilter = filterFunc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def __eq__(self):\n",
    "        return (other\n",
    "                and self.__class__ is other.__class__)\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if t.mode == 'query': # if called by query parser\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t\n",
    "            else: # == 'index' if called by indexer\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whooshSearch(df, userQuery):\n",
    "    \n",
    "    #Defining constants for the data paths ***** MODIFY ACCORDINGLY *****\n",
    "    INDEX_DIR = \"C:/UofT/4th_year/Capstone/Python_directory/schema\"\n",
    "    \n",
    "    \n",
    "    #BUILD SCHEMA ****\n",
    "    #schema has fields - piece of info for each doc in the index\n",
    "    customWordFilter = RegexTokenizer()|LowercaseFilter()|CustomFilter(nltk.stem.porter.PorterStemmer().stem)|CustomFilter(nltk.WordNetLemmatizer().lemmatize)\n",
    "\n",
    "    ixSchema = Schema(comment_ID = ID(stored=True),\n",
    "                         comment_Subreddit = ID(stored=True),\n",
    "                         #note analyzer is a wrapper for a tokenizer and zero or more filters -- i.e. allows you to combine them\n",
    "                         comment_Content = TEXT(analyzer = customWordFilter))\n",
    "  \n",
    "\n",
    "    #BUILD INDEX ****\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(INDEX_DIR):\n",
    "        os.mkdir(INDEX_DIR)\n",
    "\n",
    "\n",
    "#     if index exists - remove it\n",
    "#     #Return True if path is an existing directory.\n",
    "#     if os.path.isdir(INDEX_DIR):\n",
    "#         #Delete an entire directory tree; path must point to a directory\n",
    "#         shutil.rmtree(INDEX_DIR)\n",
    "#     #create the directory for the index\n",
    "#     os.makedirs(INDEX_DIR)\n",
    "\n",
    "    #initiate index - takes two inputs, the index directory and the schema for the index\n",
    "    ix = Index.create_in(INDEX_DIR,ixSchema)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #INDEX COMMENTS ****\n",
    "    #creating a utility writer \n",
    "    #params: index – the whoosh.index.Index to write to.\n",
    "    #period – the maximum amount of time (in seconds) between commits.\n",
    "    #limit – the maximum number of documents to buffer before committing/between commits.\n",
    "    writer = writing.BufferedWriter(ix, period=20, limit=1000)\n",
    "    try:\n",
    "        # write each file to index\n",
    "        # enumerate returns index,value index points too --> index,a[index]\n",
    "        \n",
    "        counter1 = 0\n",
    "        for row in df.iterrows():\n",
    "            index,data = row\n",
    "            writer.add_document(comment_ID = data['name'],\n",
    "                                comment_Subreddit = data['subreddit'],\n",
    "                                comment_Content = data['body'])\n",
    "            counter1 = counter1 + 1\n",
    "            \n",
    "#             if (counter1 % 100 == 0):\n",
    "#                 print(\"already indexed:\", counter1+1)\n",
    "\n",
    "    finally:\n",
    "        # save the index\n",
    "        #print(\"done indexing\")\n",
    "        # *** Note *** -> Must explictly call close() on the writer object to release the write lock and makesure uncommited changes are saved \n",
    "        writer.close()   \n",
    "      \n",
    "    \n",
    "    #PARSE USER QUERY ****\n",
    "    \n",
    "    #in the query parser --> we pass the DEFAULT field to search and the schema of the index we are searching\n",
    "    #NOTE: Users can still specify a search on a different field in the schema via --> <fieldname>: <query>\n",
    "    qp = QueryParser(\"comment_Content\", schema=ix.schema)\n",
    "\n",
    "     #Once you have a QueryParser object, you can call parse() on it to parse a query string into a query object:\n",
    "        #default query lang: \n",
    "        #If the user doesn’t explicitly specify AND or OR clauses: \n",
    "        #by default, the parser treats the words as if they were connected by AND,\n",
    "        #meaning all the terms must be present for a document to match\n",
    "        #we will change this \n",
    "        #to phrase search \"<query>\" - use quotes\n",
    "        \n",
    "    qp.add_plugin(qparser.GtLtPlugin)   \n",
    "    #qp.remove_plugin_class(qparser.PhrasePlugin)\n",
    "    qp.add_plugin(qparser.PhrasePlugin)  \n",
    "    query = qp.parse(userQuery)\n",
    "    print(\"\\n\\n Query: \")\n",
    "    print(query)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ##IMPLEMENT SEARCHER ****\n",
    "    resultsDF = pandas.DataFrame() #creates a new dataframe that's empty to store the results comment content\n",
    "    with ix.searcher(weighting = scoring.BM25F()) as searcher:\n",
    "        queryResults = searcher.search(query, limit = None)\n",
    "        print(\"Total Number of Results:\",len(queryResults))\n",
    "        print(\"Number of scored and sorted docs in this Results object:\",queryResults.scored_length())\n",
    "        for result in queryResults:\n",
    "#             print(result)\n",
    "#             print(\"\\n\",result['comment_ID'])\n",
    "            resultsDF = resultsDF.append(df.loc[df['name']== result['comment_ID']][['name','subreddit','body']])\n",
    "            \n",
    "\n",
    "    #print(dataDf.loc[dataDf['body']==comment].index.values[0])\n",
    "        \n",
    "    return resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pandas.DataFrame(columns=['name','subreddit', 'body', 'link'],index=[0,1,2,3,4,5])\n",
    "df2.loc[0] = pandas.Series({'body':\"Awesome room service\", 'name':'1', 'subreddit':'0.75', 'link':'http://google.com'})\n",
    "df2.loc[1] = pandas.Series({'body':\"Good view\", 'name':'0.5', 'subreddit':'0.75', 'link':'http://google.com'})\n",
    "df2.loc[2] = pandas.Series({'body':\"Ok view\", 'name':'0', 'subreddit':'0.5', 'link':'http://google.com'})\n",
    "df2.loc[3] = pandas.Series({'body':\"Bad room service\", 'name':'-0.5', 'subreddit':'0', 'link':'http://google.com'})\n",
    "df2.loc[4] = pandas.Series({'body':\"Terrible prices\", 'name':'-1', 'subreddit':'-0.75', 'link':'http://google.com'})\n",
    "df2.loc[5] = pandas.Series({'body':\"Disastrous prices\", 'name':'-1', 'subreddit':'-1', 'link':'http://google.com'})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(columns=['text','MI', 'PMI'],index=[0,1,2,3])\n",
    "df.loc[0] = pandas.Series({'text':\"Awesome room service\", 'MI':1, 'PMI':0.75})\n",
    "df.loc[1] = pandas.Series({'text':\"Good view\", 'MI':0.5, 'PMI':0.75})\n",
    "df.loc[2] = pandas.Series({'text':\"Ok view\", 'MI':0, 'PMI':0.5})\n",
    "df.loc[3] = pandas.Series({'text':\"Bad room service\", 'MI':-0.5, 'PMI':0})\n",
    "df.loc[4] = pandas.Series({'text':\"Terrible prices\", 'MI':-1, 'PMI':-0.75})\n",
    "df.loc[5] = pandas.Series({'text':\"Disastrous prices\", 'MI':-1, 'PMI':-1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Create Table\n",
    "class TableApp(Frame):\n",
    "\n",
    "    def __init__(self, parent, dataframe):\n",
    "        Frame.__init__(self, parent)\n",
    "        self.LoadTable(dataframe)\n",
    "        self.grid(sticky = (N,S,W,E))\n",
    "        parent.grid_rowconfigure(0, weight = 1)\n",
    "        parent.grid_columnconfigure(0, weight = 1)\n",
    "              \n",
    "    ###Get Table Values\n",
    "    def LoadTable(self, df):\n",
    "        tv = Treeview(self)\n",
    "        tv['columns'] = ('pointwisemutual', 'mutual', 'viewfull')\n",
    "        tv.heading(\"#0\", text='Key Phrase')\n",
    "        tv.column(\"#0\", anchor=\"w\", width=300)\n",
    "        tv.heading('pointwisemutual', text='PMI')\n",
    "        tv.column('pointwisemutual', anchor='center', width=50)\n",
    "        tv.heading('mutual', text='MI')\n",
    "        tv.column('mutual', anchor='center', width=50)\n",
    "        tv.heading('viewfull', text='See Full Thread')\n",
    "        tv.column('viewfull', anchor='center', width=100)\n",
    "        tv.grid(sticky = (N,S,W,E))\n",
    "        self.treeview = tv\n",
    "        self.grid_rowconfigure(0, weight = 1)\n",
    "        self.grid_columnconfigure(0, weight = 1)\n",
    "        \n",
    "        self.MItable = df\n",
    "        for (i,row) in self.MItable.iterrows():\n",
    "            self.treeview.insert('', 'end', text=row[\"body\"], values=(row[\"name\"],row[\"subreddit\"], 'View'))\n",
    "            \n",
    "        self.treeview.bind(\"<Button-1>\", self.OnClick)\n",
    "        \n",
    "    def OnClick(self, event):\n",
    "        item = self.treeview.identify('item',event.x,event.y)\n",
    "        body = self.treeview.item(item,\"text\")\n",
    "        print(\"you clicked on\", body)\n",
    "        link = df2[df2['body']==body][\"link\"].tolist()[0]\n",
    "        webbrowser.open(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Create Timeline Bar Graph\n",
    "class TimelineApp(Frame):\n",
    "\n",
    "    def __init__(self, parent):\n",
    "        Frame.__init__(self, parent)\n",
    "        self.CreateGraph()\n",
    "        self.grid(sticky = (N,S,W,E))\n",
    "        parent.grid_rowconfigure(0, weight = 1)\n",
    "        parent.grid_columnconfigure(0, weight = 1)\n",
    "        \n",
    "    def CreateGraph(self):\n",
    "        timetable = pandas.DataFrame(columns=['month','total'],index=[0,1,2])\n",
    "        timetable.loc[0] = pandas.Series({'month':\"Jan\", 'total':50})\n",
    "        timetable.loc[1] = pandas.Series({'month':\"Feb\", 'total':35})\n",
    "        timetable.loc[2] = pandas.Series({'month':\"Mar\", 'total':29})\n",
    "        timetable.loc[3] = pandas.Series({'month':\"Apr\", 'total':15})\n",
    "        timetable.loc[4] = pandas.Series({'month':\"May\", 'total':10})\n",
    "        timetable.loc[5] = pandas.Series({'month':\"Jun\", 'total':47})\n",
    "        timetable.loc[6] = pandas.Series({'month':\"Jul\", 'total':38})\n",
    "        timetable.loc[7] = pandas.Series({'month':\"Aug\", 'total':7})\n",
    "        timetable.loc[8] = pandas.Series({'month':\"Sep\", 'total':23})\n",
    "        timetable.loc[9] = pandas.Series({'month':\"Oct\", 'total':19})\n",
    "        timetable.loc[10] = pandas.Series({'month':\"Nov\", 'total':28})\n",
    "        timetable.loc[11] = pandas.Series({'month':\"Dec\", 'total':33})        \n",
    "        \n",
    "        ###month = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "                \n",
    "        c_width = 55\n",
    "        c_height = 180\n",
    "        c = Canvas(self, width=c_width, height=c_height, bg= 'gray')\n",
    "        c.pack()\n",
    "        \n",
    "        ###sizing the graph\n",
    "        ###y height = max value * y_stretch\n",
    "        y_height=timetable.total.max()\n",
    "        y_stretch = 140/y_height\n",
    "        # gap between lower canvas edge and x axis\n",
    "        y_gap = 20\n",
    "        # stretch enough to get all data items in\n",
    "        x_stretch = 15\n",
    "        x_width = 28\n",
    "        # gap between left canvas edge and y axis\n",
    "        x_gap = 20\n",
    "        \n",
    "        c.create_line(0,c_height - y_gap,550,c_height - y_gap)\n",
    "                \n",
    "        for i,row in timetable.iterrows():\n",
    "            # calculate reactangle coordinates (integers) for each bar\n",
    "            x0 = i * x_stretch + i * x_width + x_gap\n",
    "            y0 = c_height - (row[\"total\"] * y_stretch + y_gap)\n",
    "            x1 = i * x_stretch + i * x_width + x_width + x_gap\n",
    "            y1 = c_height - y_gap\n",
    "            # draw the bar\n",
    "            c.create_rectangle(x0, y0, x1, y1, fill=\"midnightblue\")\n",
    "            # put the y value above each bar\n",
    "            c.create_text(x1, y0, anchor=SE, text=row[\"total\"])\n",
    "            c.create_text(x1, c_height-2, anchor=SE, text=row[\"month\"])\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Main Application\n",
    "class App(tkinter.Tk):\n",
    "\n",
    "    ###Initialize things\n",
    "    def __init__(self, parent):\n",
    "        tkinter.Tk.__init__(self, parent)\n",
    "        self.parent = parent\n",
    "        self.initialize()\n",
    "    \n",
    "    ###Actually initialize the program\n",
    "    def initialize(self):\n",
    "    \n",
    "        ###Create the main container frames\n",
    "        firstframe = Frame(self, bg='yellowgreen', width = 1000, height=50, padx=3, pady=3)\n",
    "        secondframe = Frame(self, bg='gray', width=1000, height=100, padx=3, pady=3)\n",
    "        thirdframe = Frame(self, bg='white', width = 1000, height = 100, padx=3, pady=3)\n",
    "        self.fourthframe = Frame(self, bg='lavender', width = 1000, height = 100, padx=3, pady=3)\n",
    "        fifthframe = Frame(self, bg='yellowgreen', width = 1000, height = 300, padx=3, pady=3)\n",
    "        sixthframe = Frame(self, bg='gray', width = 1000, height = 300, padx=3, pady=3)\n",
    "\n",
    "        ###layout all of the main containers\n",
    "        firstframe.grid(row=0, sticky=\"ew\")\n",
    "        secondframe.grid(row=1, sticky=\"nsew\")\n",
    "        thirdframe.grid(row=3, sticky=\"ew\")\n",
    "        self.fourthframe.grid(row=4, sticky=\"ew\")\n",
    "        fifthframe.grid(row=5, sticky=\"ew\")\n",
    "        sixthframe.grid(row=6, sticky=\"ew\")\n",
    "        \n",
    "        ###first frame = search box\n",
    "        submit = Button(firstframe, text =\"Search\", background=\"yellowgreen\", command=self.OnButtonClick)\n",
    "        \n",
    "        ###self.entryVariable is the search item\n",
    "        self.entryVariable = tkinter.StringVar()\n",
    "        self.entry = tkinter.Entry(firstframe,textvariable=self.entryVariable)\n",
    "        self.entry.bind(\"<Return>\", self.OnPressEnter)\n",
    "        self.entryVariable.set(\"\")\n",
    "        \n",
    "        ###top frame layout\n",
    "        submit.grid(row=0, column=1)\n",
    "        self.entry.grid(row=0, column=0, sticky='EW')\n",
    "\n",
    "        ###second frame = summary stats\n",
    "        statslabel1= Label(secondframe, text=\"Total Comments\", bg=\"gray\")\n",
    "        statslabel2= Label(secondframe, text=\"Total Subreddits\", bg=\"gray\")\n",
    "        statslabel3= Label(secondframe, text=\"Total Positive\", bg=\"gray\", fg=\"darkgreen\")\n",
    "        statslabel4= Label(secondframe, text=\"Total Neutral\", bg=\"gray\", fg=\"yellow\")\n",
    "        statslabel5= Label(secondframe, text=\"Total Negative\", bg=\"gray\", fg=\"red\")\n",
    "        \n",
    "        self.statResult1 = tkinter.StringVar()\n",
    "        self.statResult2 = tkinter.StringVar()        \n",
    "        self.statResult3 = tkinter.StringVar()        \n",
    "        self.statResult4 = tkinter.StringVar()        \n",
    "        self.statResult5 = tkinter.StringVar()\n",
    "        \n",
    "        self.statResult1.set(u\"0\")\n",
    "        self.statResult2.set(u\"0\")\n",
    "        self.statResult3.set(u\"0\")\n",
    "        self.statResult4.set(u\"0\")\n",
    "        self.statResult5.set(u\"0\")\n",
    "        \n",
    "        resultlabel1 = Label(secondframe,textvariable=self.statResult1, bg=\"gray\")\n",
    "        resultlabel2 = Label(secondframe,textvariable=self.statResult2, bg=\"gray\")\n",
    "        resultlabel3 = Label(secondframe,textvariable=self.statResult3, bg=\"gray\")\n",
    "        resultlabel4 = Label(secondframe,textvariable=self.statResult4, bg=\"gray\")\n",
    "        resultlabel5 = Label(secondframe,textvariable=self.statResult5, bg=\"gray\")\n",
    "\n",
    "        ###second frame layout\n",
    "        statslabel1.grid(row=0, column=0)\n",
    "        statslabel2.grid(row=0, column=2)\n",
    "        statslabel3.grid(row=1, column=0, sticky=\"w\")\n",
    "        statslabel4.grid(row=1, column=2)\n",
    "        statslabel5.grid(row=1, column=4)\n",
    "\n",
    "        resultlabel1.grid(row=0, column=1)\n",
    "        resultlabel2.grid(row=0, column=3)\n",
    "        resultlabel3.grid(row=1, column=1)\n",
    "        resultlabel4.grid(row=1, column=3)\n",
    "        resultlabel5.grid(row=1, column=5)\n",
    "\n",
    "        ###third frame = Key Phrase with PMI and MI headers\n",
    "        keyphraselabel = Label(thirdframe, text=\"Key Phrases for\")\n",
    "        keyphrasesearch = Label(thirdframe, textvariable=self.entryVariable)\n",
    "        keyphraseshow = Label(thirdframe, text=\"Show: \")\n",
    "        \n",
    "        choice = StringVar()\n",
    "        choice.set(\"all\")\n",
    "        self.RB1 = Radiobutton(thirdframe, text=\"all\", variable=choice, value=\"all\", state=\"disabled\")\n",
    "        self.RB2 = Radiobutton(thirdframe, text=\"pos\", variable=choice, value=\"pos\", state=\"disabled\")\n",
    "        self.RB3 = Radiobutton(thirdframe, text=\"neg\", variable=choice, value=\"neg\", state=\"disabled\")\n",
    "        \n",
    "        showallcomments = Button(thirdframe, text =\"Show All Comments\", command=self.SecondWindow)\n",
    "                \n",
    "        ###third frame layout\n",
    "        keyphraselabel.grid(row=0,column=0)\n",
    "        keyphrasesearch.grid(row=0,column=1)\n",
    "        keyphraseshow.grid(row=0,column=3)\n",
    "        self.RB1.grid(row=0, column=4)\n",
    "        self.RB2.grid(row=0, column=5)\n",
    "        self.RB3.grid(row=0, column=6)\n",
    "        showallcomments.grid(row=0, column=7)\n",
    "        \n",
    "        ###fourthframe = Table\n",
    "        self.table = TableApp(self.fourthframe, df2)\n",
    "        \n",
    "        ###fifth frame = Timeline Header\n",
    "        timelinehead = Label(fifthframe, text=\"Comment Frequency Timeline\", bg=\"yellowgreen\")\n",
    "        \n",
    "        timelinehead.grid(row=0, column=0)\n",
    "        \n",
    "        ###sixth frame = Actual Timeline\n",
    "        TimelineApp(sixthframe)\n",
    "        \n",
    "    ###Click Submit Button and show all search\n",
    "    def OnButtonClick(self):\n",
    "        print(\"OnButtonClick\")\n",
    "        searchTable = whooshSearch(df2,self.entryVariable.get())\n",
    "        print(searchTable)\n",
    "        self.RB1.configure(state=\"normal\")\n",
    "        self.RB2.configure(state=\"normal\")\n",
    "        self.RB3.configure(state=\"normal\")\n",
    "        self.RB1.select()\n",
    "        #self.table = TableApp(self.fourthframe, searchTable)\n",
    "        self.fourthframe.destroy()\n",
    "        self.fourthframe = Frame(self, bg='lavender', width = 1000, height = 100, padx=3, pady=3)\n",
    "        self.fourthframe.grid(row=4, sticky=\"ew\")\n",
    "        #print(\" \".join(searchTable[\"body\"].tolist()))\n",
    "        #keyPhraseTable = get_KeyPhrases(\" \".join(searchTable[\"body\"].tolist()))\n",
    "        #print(keyPhraseTable)\n",
    "        #TableApp(self.fourthframe, keyPhraseTable)\n",
    "        TableApp(self.fourthframe, searchTable)\n",
    "        self.currentTable = searchTable\n",
    "        \n",
    "    ###search function to be implemented\n",
    "        \n",
    "    ###summary statistics\n",
    "\n",
    "    ###Click Enter and show all search\n",
    "    def OnPressEnter(self,event):\n",
    "        print(\"OnPressEnter\")\n",
    "        searchTable = whooshSearch(df2,self.entryVariable.get())\n",
    "        self.RB1.configure(state=\"normal\")\n",
    "        self.RB2.configure(state=\"normal\")\n",
    "        self.RB3.configure(state=\"normal\")\n",
    "        self.RB1.select()\n",
    "        self.fourthframe.destroy()\n",
    "        self.fourthframe = Frame(self, bg='lavender', width = 1000, height = 100, padx=3, pady=3)\n",
    "        self.fourthframe.grid(row=4, sticky=\"ew\")\n",
    "        TableApp(self.fourthframe, searchTable)\n",
    "        self.currentTable = searchTable\n",
    "        \n",
    "    ###Create Second Window    \n",
    "    def SecondWindow(self):\n",
    "        \n",
    "        child = tkinter.Toplevel(self)\n",
    "        child.title(\"Comments\")\n",
    "        ###Create the main container frames\n",
    "        windowfirst = Frame(child, bg='yellowgreen', width = 1000, height=50, padx=3, pady=3)\n",
    "        windowsecond = Frame(child, bg='gray', width=1000, height=100, padx=3, pady=3)\n",
    "\n",
    "        ###layout all of the main containers\n",
    "        windowfirst.pack()\n",
    "        windowsecond.pack()\n",
    "\n",
    "        ##comment \n",
    "        ctr_left = Frame(windowsecond, bg='red', width=500, height=300, padx=3, pady=3)\n",
    "        ctr_right = Frame(windowsecond, bg='brown', width=500, height=300, padx=3, pady=3)\n",
    "\n",
    "        ctr_left.grid(row=0, column = 0, sticky=\"ns\")\n",
    "        ctr_right.grid(row=0, column = 2, sticky=\"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webbrowser.open('https://www.reddit.com/r/'+'shopify'+'/comments/'+'558i3t') # subreddit and link_id\n",
    "# scroll down???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webbrowser.open('https://www.reddit.com'+'/r/cscareerquestions/comments/4h6zt6/waterloo_mechatronics_vs_cs_coop/'+'d2nxm5t') #permalink and comment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = App(None)\n",
    "root.title('Shopify Reddit Analyzer')\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
